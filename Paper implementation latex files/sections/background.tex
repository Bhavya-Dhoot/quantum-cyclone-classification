%%% ============================================================
%%% SECTION 3: BACKGROUND
%%% ============================================================
\section{Background}\label{sec:background}

The method proposed in this paper sits at the intersection of several ideas from classical machine learning and quantum information. To make the exposition self-contained, we review support vector machines and the kernel trick, introduce quantum feature maps as a general concept, and then focus on IQP circuits---the particular circuit family we use---and the complexity-theoretic results that make them interesting.

\subsection{Support Vector Machines and Kernel Methods}\label{sec:svm}

At their core, support vector machines~\cite{cortes1995, vapnik1995} solve a deceptively simple optimisation problem: find the hyperplane that separates two classes of labelled data with the widest possible margin. Given examples $\{(\mathbf{x}_i, y_i)\}_{i=1}^{N}$, where each $\mathbf{x}_i \in \mathbb{R}^d$ and $y_i \in \{-1, +1\}$, the SVM solves
\begin{equation}\label{eq:svm}
    \min_{\mathbf{w}, b} \frac{1}{2}\norm{\mathbf{w}}^2 + C \sum_{i=1}^{N} \max(0, 1 - y_i(\mathbf{w}^\top \mathbf{x}_i + b)),
\end{equation}
with the regularisation constant $C > 0$ mediating between a wide margin and low training error.

The real power emerges through the kernel trick, which dates back at least to Aizerman in 1964 but was popularised in the SVM context by Boser, Guyon, and Vapnik. A feature map $\phi: \mathbb{R}^d \to \mathcal{F}$ lifts the data into a (potentially infinite-dimensional) space~$\mathcal{F}$, and the kernel function
\begin{equation}\label{eq:kernel}
    k(\mathbf{x}, \mathbf{x}') = \langle \phi(\mathbf{x}), \phi(\mathbf{x}') \rangle_{\mathcal{F}}
\end{equation}
evaluates inner products in that space without ever materialising the feature vectors themselves. Widely used kernels include the linear kernel $k(\mathbf{x}, \mathbf{x}') = \mathbf{x}^\top \mathbf{x}'$, the polynomial kernel $k(\mathbf{x}, \mathbf{x}') = (\gamma \mathbf{x}^\top \mathbf{x}' + r)^p$, and the RBF kernel $k(\mathbf{x}, \mathbf{x}') = \exp(-\gamma \norm{\mathbf{x} - \mathbf{x}'}^2)$~\cite{hastie2009}. The critical question, always, is whether the geometry that a kernel imposes on the data actually matches the structure of the classification problem. When it does, SVMs are among the most effective classifiers available; when it does not, no amount of regularisation tuning will compensate.

For the three-class cyclone intensity problem considered here, we adopt one-versus-rest decomposition: $K$ binary SVMs are trained, one per class, and a test point goes to whichever class yields the highest decision function score.

\subsection{Quantum Feature Maps}\label{sec:quantum-feature-maps}

In this work, the IQP feature map transforms each normalized feature vector into a six-qubit quantum state using parameterized $R_Z$ and controlled-phase gates. Formally, a parameterised unitary $U(\mathbf{x})$ acts on the computational zero state~\cite{havlicek2019, schuld2019}:
\begin{equation}\label{eq:quantum-feature-map}
    \ket{\phi(\mathbf{x})} = U(\mathbf{x}) \ket{0}^{\otimes n}.
\end{equation}
Two data points $\mathbf{x}$ and $\mathbf{x}'$ are compared through their state overlap,
\begin{equation}\label{eq:quantum-kernel}
    k_Q(\mathbf{x}, \mathbf{x}') = \abs{\braket{\phi(\mathbf{x})}{\phi(\mathbf{x}')}}^2,
\end{equation}
which functions as the quantum kernel.

Schuld~\cite{schuld2021kernel} proved a result that clarifies the landscape considerably: every supervised quantum model that returns measurement expectation values is implicitly a kernel method, with the Hilbert-space inner product playing the role of the kernel and the encoding unitary $U(\mathbf{x})$ playing the role of the feature map. So the entire inductive bias of a quantum classifier---what it can and cannot learn---collapses onto the design of that single circuit.

This puts a premium on getting the circuit right. Too shallow, and the kernel is efficiently simulable---one could have stuck with a classical RBF kernel and saved the trouble~\cite{schuld2021effect}. Too deep, and exponential concentration kicks in: $k_Q(\mathbf{x}, \mathbf{x}')$ approaches zero for almost every pair of inputs, and the Gram matrix becomes a useless identity-like blob~\cite{thanasilp2024}. The practical design space, then, occupies a narrow corridor: circuits complex enough to be classically intractable, yet structured enough to avoid concentration. IQP circuits, as we argue next, are well-positioned within that corridor.

\subsection{Instantaneous Quantum Polynomial (IQP) Circuits}\label{sec:iqp}

Shepherd and Bremner~\cite{shepherd2009} defined IQP circuits as quantum computations in which all gates commute---an apparently severe restriction that turns out to have surprisingly powerful consequences. The canonical circuit on $n$ qubits is
\begin{equation}\label{eq:iqp-circuit}
    U_{\text{IQP}}(\boldsymbol{\theta}) = H^{\otimes n} \cdot D(\boldsymbol{\theta}) \cdot H^{\otimes n},
\end{equation}
with $H^{\otimes n}$ denoting Hadamard gates on every qubit and $D(\boldsymbol{\theta})$ a diagonal unitary parameterised by~$\boldsymbol{\theta}$. The commutativity of the gates inside $D(\boldsymbol{\theta})$ is the reason for the word \emph{instantaneous}: informally, all the ``action'' in the circuit happens in a single diagonal step, bracketed by Hadamard layers that create and then interfere superpositions.

Bremner, Jozsa, and Shepherd~\cite{bremner2010} proved that sampling from the output distribution of such a circuit is classically intractable---more precisely, it would require the polynomial hierarchy to collapse to its third level, a consequence that essentially no complexity theorist expects. The result was strengthened to average-case hardness by Bremner, Montanaro, and Shepherd~\cite{bremner2016}, ruling out even approximate classical simulation under mild anti-concentration assumptions.

The diagonal unitary decomposes neatly into single-qubit and two-qubit pieces:
\begin{equation}\label{eq:diagonal}
    D(\boldsymbol{\theta}) = \prod_{j=1}^{n} R_Z(\theta_j) \prod_{j < k} \text{CP}(\theta_{jk}),
\end{equation}
where $R_Z(\theta_j) = \exp(-i\theta_j Z_j / 2)$ are phase rotations and $\text{CP}(\theta_{jk}) = \exp(-i\theta_{jk} Z_j Z_k / 2)$ are controlled-phase gates.

Setting $\boldsymbol{\theta}$ to be a function of a classical input~$\mathbf{x}$ turns the IQP circuit into a data-encoding feature map. The hardness of classical simulation then transfers directly to the kernel: were the kernel values classically computable, one could use them to reconstruct IQP output distributions efficiently, contradicting~\cite{bremner2010}. This argument provides not just a heuristic motivation but a formal, complexity-theoretic reason to expect that IQP-based kernels access structure in the data that classical kernels cannot.

\input{figures/iqp-circuit}

\subsection{Quantum Kernel Estimation}\label{sec:kernel-estimation}

Operationally, the kernel $k_Q(\mathbf{x}, \mathbf{x}')$ is computed by preparing the composite state $U(\mathbf{x})^\dagger U(\mathbf{x}') \ket{0}^{\otimes n}$ and then measuring the probability of collapsing back to the all-zeros bitstring:
\begin{equation}\label{eq:kernel-estimation}
    k_Q(\mathbf{x}, \mathbf{x}') = \abs{\bra{0}^{\otimes n} U(\mathbf{x})^\dagger U(\mathbf{x}') \ket{0}^{\otimes n}}^2.
\end{equation}

In statevector simulation, this probability is exact. On physical hardware, it would be estimated from repeated measurements, with statistical error scaling as $1/\sqrt{N_{\text{shots}}}$. We use statevector simulation throughout this study so that the influence of feature-map design can be isolated from hardware-specific noise; an analysis that incorporates gate errors and decoherence is deferred to future work.
