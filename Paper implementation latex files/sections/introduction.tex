%%% ============================================================
%%% SECTION 1: INTRODUCTION
%%% ============================================================
\section{Introduction}\label{sec:introduction}

Tropical cyclones, quantified by specific intensity categories tracking sustained surface wind variables, cause massive global socioeconomic disruption. Single storm events inflict heavy casualties and generate billions in economic damage~\cite{emanuel2005}. Observational datasets and climate projections reveal a specific statistical trend. The overall frequency of cyclone genesis remains relatively stable. However, the proportion of storms reaching high Saffir--Simpson categories shows a measurable increase. Warming ocean surface temperatures drive this specific intensification trend~\cite{knutson2010}. Classification of storm intensity demands high operational reliability. Forecasters must differentiate quickly between tropical depressions, tropical storms, and severe hurricanes. Emergency management systems depend on this categorisation for evacuation protocols. Resource staging and actuarial risk modelling require accurate classification algorithms.

The Dvorak technique provides the historical foundation for intensity estimation. This method requires human analysts~\cite{simpson1974}. Forecasters examine satellite imagery and assign intensity scores based on cloud morphology. This manual approach introduces unavoidable inter-analyst variability. Machine learning models such as CNN-based cyclone intensity predictors and SVM-based classification systems trained on IBTrACS and ERA5 datasets have demonstrated high predictive performance. Researchers frequently deploy deep convolutional networks and recurrent architectures for storm trajectory mapping~\cite{chen2020cyclone, alemany2019, mcgovern2017}. Classical support vector machines (SVMs) exhibit strong utility in this domain. Vapnik's maximum-margin formulation guarantees robust boundary formation even when training samples remain sparse~\cite{cortes1995, vapnik1995}.

Classical kernel methods operate under strict functional limitations. Tropical convection physics dictate complex correlations between variables. Sea-level pressure, sustained wind speed, vertical shear, and humidity interact continuously. Standard polynomial or radial basis function (RBF) kernels impose rigid topological constraints on these variables. The classifier succeeds only if the assumed kernel function accidentally aligns with the actual atmospheric data geometry.

Quantum computing presents an alternative data embedding strategy. Parameterised quantum circuits transform classical data vectors into high-dimensional quantum states. State overlaps compute the algorithmic similarity metric. The resulting kernel matrix resides in an exponentially large Hilbert space~\cite{havlicek2019, schuld2019}. Formal mathematical proofs exist showing quantum kernels can outperform classical equivalents under specific data conditions~\cite{liu2021, huang2021}. Designing the feature map requires engineering precision. Shallow circuits remain classically simulable. Deep, highly entangled circuits trigger exponential concentration and destroy all feature signal.

Instantaneous Quantum Polynomial (IQP) circuits provide the required structural balance. Shepherd and Bremner proposed these specific circuits~\cite{shepherd2009}. The architecture sandwiches a diagonal unitary block between Hadamard layers. All gates within the central diagonal block commute. Bremner, Jozsa, and Shepherd demonstrated a critical property regarding these computing circuits~\cite{bremner2010}. Classical hardware cannot sample IQP output distributions efficiently without forcing the polynomial hierarchy to collapse. Average-case hardness proofs extend this fundamental theoretical result~\cite{bremner2016}. Machine learning models benefit from this exact computational property. A kernel built from IQP mappings fundamentally resists classical simulation.

We evaluate this complexity-theoretic hardness against empirical meteorological data. We construct an SVM framework using a bespoke IQP feature map. The IBTrACS database supplies all training and test observations~\cite{knapp2010}. Our specific contributions follow:

\begin{enumerate}
    \item A structural translation of atmospheric variables into a parameterised IQP quantum circuit. Single-qubit sequences manage individual predictors. Two-qubit controlled-phase gates process physical interactions between pressure, wind speed, latitude, and translational velocity.
    \item Full execution of a hybrid quantum--classical data pipeline requiring no variational optimisation.
    \item Direct empirical benchmarking. We test the IQP method against linear, polynomial, and RBF classical kernels. We include an entangled quantum ZZFeatureMap as a secondary control.
    \item Detailed analysis of geometric kernel alignment. We dissect classification accuracy using precision, recall, and exact confusion matrix structures.
\end{enumerate}

Section~\ref{sec:related-work} outlines historical model implementations. Section~\ref{sec:background} details the mathematical foundation for SVMs and IQP circuits. Section~\ref{sec:methodology} describes dataset preparation and feature encoding. Section~\ref{sec:experiments} presents the explicit classification output. Section~\ref{sec:discussion} analyses error clustering and simulation limits. Section~\ref{sec:conclusion} concludes.
